{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8UtnfXdtQsbx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class LogisticRegression:\n",
        "    \"\"\"\n",
        "    1. Initialise weight and bias with random.randn according to number of features and number of classes\n",
        "    2. Encode the target feature using one hot encoder (Convert 1D into multi dimension)\n",
        "    3. Calculate the gradient descent and update weight matrix:\n",
        "        1. Project Z\n",
        "        2. With some activation function calculate probability of Z (Softmax):\n",
        "            Softmax e^x/ sum(e^x)\n",
        "        3. Calculate Error\n",
        "        4. CAlculate with dot product of error of X with weight\n",
        "        5. Db with summation of error in row\n",
        "    4. Predict using updated weight and bias , calculate probability with sigmoid function and take argmax\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, max_iters=1000):\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "\n",
        "    def softmax(self, vector):\n",
        "        \"\"\"\n",
        "        Helper class to calculate sigmoid value\n",
        "        :param vector:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        e = np.exp(vector)\n",
        "        return (e / np.sum(e, axis=1, keepdims=True))\n",
        "\n",
        "    def oneHotEncoder(self, nSamples, y):\n",
        "        \"\"\"\n",
        "        Helper function for one hot encoding\n",
        "        :param nSamples:\n",
        "        :param y:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        y_encode = np.zeros((nSamples, len(np.unique(y))))\n",
        "        for j in range(nSamples):\n",
        "            y_encode[j][y[j]] = 1\n",
        "        return y_encode\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Update weight and bias according to Logistic regression function\n",
        "        :param X:\n",
        "        :param y:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        nSamples, nFeatures = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "        loss = []\n",
        "        # np.random.randn(123)\n",
        "        self.bias = np.random.randn(num_classes)  # initializing random normalized bias (For each classes)\n",
        "        self.weights = np.random.randn(nFeatures, num_classes)  # initializing random normalized weights (For Each Num features, Class)\n",
        "        y_encode = self.oneHotEncoder(nSamples, y)  # Encode to one hot encoder\n",
        "        # iterating to max_epochs\n",
        "        for i in range(self.max_iters):\n",
        "            Z = np.dot(X, self.weights) + self.bias # calculate Z\n",
        "            prob_y = self.softmax(Z) # get probability using softmax\n",
        "            error = prob_y - y_encode # residual error\n",
        "            dw = (1 / nSamples) * np.dot(X.T, error) # dw of weight\n",
        "            db = (1 / nSamples) * np.sum(error, axis=0) # db for bias\n",
        "            self.weights -= self.learning_rate * dw # update weight \n",
        "            self.bias -= self.learning_rate * db # undate bias\n",
        "            loss.append(-np.mean(np.dot(y, np.log(prob_y))) - np.dot((1 - y) ,np.log(1-prob_y))) # calculate loss\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicting the value taking the final best update weight and bias\n",
        "        :param X: input\n",
        "        :return: return predicted value\n",
        "        \"\"\"\n",
        "        Z = np.dot(X, self.weights) + self.bias # get z\n",
        "        prob_y = self.softmax(Z) # get prob\n",
        "        return np.argmax(prob_y, axis=1) # get highest  prob clas\n",
        "\n",
        "    def score(self, y_pred, y):\n",
        "        \"\"\"\n",
        "        Get MSE score btw actual and predicted\n",
        "        :param y_pred:\n",
        "        :param y:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return np.mean(y_pred == y) # get mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixeM-sZNg-JZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, stratify=iris.target)\n",
        "# Preprocess data\n",
        "scaler = StandardScaler() # use standar scaler\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "logreg = LogisticRegression() # get model\n",
        "logreg.fit(X_train[:,2:4], y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = logreg.predict(X_test[:,2:4]) # Petal Feature\n",
        "\n",
        "acc = logreg.score(y_pred,y_test)\n",
        "acc = acc * 100\n",
        "\n",
        "print(f'Test accuracy: {acc:.2f}') # get accuaracy\n",
        "\n",
        "# Figure Plot\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "plot_decision_regions(X_train[:,2:4], y_train, clf=logreg, legend=1,ax=ax)\n",
        "plt.xlabel('petal length (cm)')\n",
        "plt.ylabel('petal width (cm)')\n",
        "plt.title('Logistic Regression on Iris - Petal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbHbT4tafqLI"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, stratify=iris.target)\n",
        "# Preprocess data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train[:,:2], y_train) # sepal Features \n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = logreg.predict(X_test[:,:2])\n",
        "\n",
        "acc = logreg.score(y_pred,y_test) # get Accuaracy\n",
        "acc = acc * 100\n",
        "print(f'Test accuracy: {acc:.2f}')\n",
        "\n",
        "# plot decission boundary\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "plot_decision_regions(X_train[:,:2], y_train, clf=logreg, legend=1,ax=ax)\n",
        "plt.xlabel('Sepal length (cm)')\n",
        "plt.ylabel('Sepal width (cm)')\n",
        "plt.title('Logistic Regression on Iris - Sepal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PG8rfZs8mQRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c029c238-7942-4c12-ede7-022a3331bd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 80.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, stratify=iris.target)\n",
        "# Preprocess data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train) # ALl Features\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "acc = logreg.score(y_pred,y_test) # get MSE\n",
        "acc = acc*100\n",
        "print(f'Test accuracy: {acc:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eY1X7yNLmhXq"
      },
      "outputs": [],
      "source": [
        "class LDA:\n",
        "    \"\"\"\n",
        "    Powerfull tool for dimensionality reduction and classification\n",
        "    1.Calculate Mean Vector for each class\n",
        "    2. Calculate covariance vector  to measure the spread of the data around the mean vector\n",
        "    3. Compute between-class scatter matrix (Variance between mean vector of each class) Sum of difference of each class and overall mean vactor\n",
        "    4. Compute within-class scatter matrix (Variance between within class) sum of covariance matrix for each class\n",
        "    5. Compute eigenvectors (in the direction where the data varies most)  and eigenvalues (Amount of variance in each of those direction) of (Sw^-1)Sb (product of inverse of within class scatter matrix and between class scatter matrix\n",
        "    6. Sort these eigenvectors in descending order. select top k eigen vectors of new vector space. L is the number of new features\n",
        "    7. create Projection Matrix which transform original into new feature\n",
        "    8. Project samples in this new feature space .(Multiply centered feature matrix transpose with projection matrix)\n",
        "    9. Use threshold to classify each sample. Threshold determined by maximizing separation between mean of two classes\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.w = None\n",
        "        self.b = None\n",
        "        self.shared_covariance = None\n",
        "        self.class_means = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        This Is the fit function which create mean vector, shared covariance, class shared scatter matrix, \n",
        "        within class scatter matrix eigen values and eigen vectors\n",
        "         :param X: :param y: :return: \n",
        "        \"\"\"\n",
        "        # Compute class means\n",
        "        self.class_means = [np.mean(X[y == c], axis=0) for c in np.unique(y)] # for each Class C\n",
        "        # Compute shared covariance matrix\n",
        "        self.shared_covariance = np.cov(X.T) # get Cov using Np. cov\n",
        "        # Compute between-class scatter matrix\n",
        "        Sb = np.zeros((X.shape[1], X.shape[1])) # initialize to to store Sb\n",
        "        for i, mean_vec in enumerate(self.class_means):\n",
        "            n = X[y == i + 1, :].shape[0] \n",
        "            mean_vec = mean_vec.reshape(X.shape[1], 1) # mean vector\n",
        "            overall_mean = np.mean(X, axis=0).reshape(X.shape[1], 1)\n",
        "            Sb += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T) #  Variance between mean vector of each class\n",
        "        # Compute within-class scatter matrix\n",
        "        Sw = self.shared_covariance # store within class scatter matrix\n",
        "        # Compute eigenvectors and eigenvalues of (Sw^-1)Sb\n",
        "        eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(Sw).dot(Sb)) # get direction using linalag of eigen values\n",
        "        # Sort eigenvectors in descending order of eigenvalues\n",
        "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i]) for i in range(len(eig_vals))] # make PAir\n",
        "        eig_pairs.sort(key=lambda x: x[0], reverse=True) # sort it in decending order\n",
        "        # Choose the first eigenvector as the projection vector\n",
        "        self.w = eig_pairs[0][1].reshape(X.shape[1], 1) # choose first as projection\n",
        "        # Compute the bias term\n",
        "        self.b = -self.w.T.dot(np.mean(self.class_means, axis=0).reshape(X.shape[1], 1)) # Bias Term\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the y value using the update weight matrix and bias matrix\n",
        "        :param X:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        y_pred = []\n",
        "        for x in X:\n",
        "            y_pred.append(np.sign(self.w.T.dot(x.reshape(X.shape[1], 1)) + self.b)) # project X in using projection vector and bias\n",
        "        return np.array(y_pred).flatten() # flatten the array\n",
        "\n",
        "    def score(self, y_pred, y):\n",
        "        \"\"\"\n",
        "        MSE of the output and actual y\n",
        "        :param y_pred:\n",
        "        :param y:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return np.mean(y_pred == y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mrYzVdY_1lN"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, stratify=iris.target)\n",
        "# Preprocess data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "pipeline = make_pipeline(scaler, LDA())\n",
        "# Train Linear Discriminant analysis model\n",
        "lda = LDA()\n",
        "lda.fit(X_train[:,2:4], y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = lda.predict(X_test[:,2:4])\n",
        "\n",
        "# cross validation score \n",
        "scores =  cross_val_score(pipeline, X_train[:,2:4], y_train,cv=5, scoring='accuracy')\n",
        "\n",
        "print('LDA CV acc', np.mean(scores), '+/-' , np.std(scores))\n",
        "acc = lda.score(y_pred,y_test)\n",
        "acc  = acc*100\n",
        "print(f'Test accuracy: {acc:.2f}')\n",
        "# Plot the figure\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "plot_decision_regions(X_train[:,2:4], y_train, clf=lda, legend=1,ax=ax)\n",
        "plt.xlabel('petal length (cm)')\n",
        "plt.ylabel('petal width (cm)')\n",
        "plt.title('LDA on Iris - Petal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYsiDcjhAVaV"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, stratify=iris.target)\n",
        "# Preprocess data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "pipeline = make_pipeline(scaler, LDA()) # make pipeline\n",
        "# Train Linear Discriminant analysis model\n",
        "lda = LDA()\n",
        "lda.fit(X_train[:,:2], y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = lda.predict(X_test[:,:2])\n",
        "\n",
        "# cross validation score \n",
        "scores =  cross_val_score(pipeline, X_train[:,:2], y_train,cv=5, scoring='accuracy')\n",
        "\n",
        "print('LDA CV acc', np.mean(scores), '+/-' , np.std(scores))\n",
        "acc = lda.score(y_pred,y_test)\n",
        "acc = acc*100\n",
        "print(f'Test accuracy: {acc:.2f}')\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "plot_decision_regions(X_train[:,:2], y_train, clf=lda, legend=1,ax=ax)\n",
        "plt.xlabel('Sepal length (cm)')\n",
        "plt.ylabel('Sepal width (cm)')\n",
        "plt.title('LDA on Iris - Sepal')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvLkzIv_eJUl"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.plotting import plot_decision_regions\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, stratify=iris.target)\n",
        "# Preprocess data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "pipeline = make_pipeline(scaler, LDA())\n",
        "# Train Linear Discriminant analysis model\n",
        "lda = LDA()\n",
        "lda.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate on test set\n",
        "y_pred = lda.predict(X_test)\n",
        "\n",
        "# cross validation score \n",
        "scores =  cross_val_score(pipeline, X_train, y_train,cv=5, scoring='accuracy')\n",
        "\n",
        "print('LDA CV acc', np.mean(scores), '+/-' , np.std(scores))\n",
        "acc = lda.score(y_pred,y_test)\n",
        "acc = acc*100\n",
        "print(f'Test accuracy: {acc:.2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "      Model Features      Logistic Regression MSE\t Linear Discriminant Analysis MSE\n",
        "    1\tPetal Features\t      93.33\t33.33\n",
        "    2\tSepal Features\t      80.00\t26.67\n",
        "    3\tAll Features\t        93.33\t33.33"
      ],
      "metadata": {
        "id": "7PGQYyzlfs8m"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ddb0416dc1e72a054707c43a0e157fd8cabc2b191049f88beb2919e337322288"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}